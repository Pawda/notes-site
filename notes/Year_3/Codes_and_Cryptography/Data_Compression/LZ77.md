---
title: LZ77
lecturer: Max
---

# Limitations of statistical compression

LZ77 can look at the whole message and determine the frequency of each symbol. Compact codes for memoryless sources are guaranteed to be optimal on average, but we may not have an average message. If the encoding is not determined in advance, but is memory dependent, then we must transmit the code as well as the encoded message.

## Source modelling

Zipf showed[^1] that the distribution of words in natural language satisfies the law described below.

[^1]: G. Zipf,Human behavior and the principle of least effort, Addison-Wesley, Reading, MA, 1949

Suppose a natural language has N words, sorted in non-increasing frequency $(p(1)\geqslant ...\geqslant p(r)...\geqslant p(N))$ then the probability of the word at the r-th rank is

$$
p(r)=\dfrac{\mu}{r}
$$

with

$$
\mu\approx \dfrac{1}{\log_eN+\gamma}
$$

where $\gamma\approx 0.577..$ is the Euler-Mascheroni constant

### -order models

The first order model of English is each character with the probability of their use, so a random sample from this model will have many vowels. The second order looks for common pairs, for example Q is very likely to be followed by U. This generates a more accurate model, and you can keep going with the models to get increasing accuracy.

You could use Huffman coding with these models but there are two main issues:

-   The alphabet explodes in size as they are $26^n$
-   The model is only appropriate for a particular sort of text so wouldn't work well for all languages and couldn't work for different alphabets

# Lempel-Ziv

The main idea of dictionary based compression is to construct a dictionary of commonly used subsequences and refer to this to build the coded message. The main idea of LZ77 is to use the message itself as a dictionary.

The encoding scans the message from first to last character. For implementation purposes, it uses a sliding window of size _W_ and a look-ahead buffer of size _L_.

Consider the message $m_1..m_n$. When encoding at character _i_, look for the largest _l_ such that the first _l_ characters of the look-ahead buffer match _l_ consecutive characters in the sliding window, i.e.

$$
m_i...m_{i+l-1}=m_{i-d}...m_{i-d+l-1}
$$

where $d\leqslant W$ and $l\leqslant L$. Append the coded message with $(d,l,m_{i+l})$, which it interprets as the instruction

> Print out $m_{i-d}...m_{i-d+l-1}m_{i+l}$ (the _l_ successive characters of _m_ starting from _d_ positions ago, and them $m_{i+l}$)

## How much space do we need?

Each triple in the encoding includes $d\leqslant W, l\leqslant L$ and a character. For ASCII, the character takes 8 bits. In total we need

$$
\log_2(W+1)=\log_2(L+1)+8
$$

bits to encode a triple

Typical values are $W=2^{16}-1=65535$ and $L=2^8-1=255$, so we need 16+8+8 bits per triple, i.e. 4 bytes. This much can be wasteful, especially if the value of l is very low.